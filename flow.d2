direction: down

# Trigger Events
Trigger: {
  pull_request: "pull_request to v2"
  workflow_dispatch: "workflow_dispatch"
}

# GitHub Actions Workflow
GHA: "codex-pr-review.yaml" {
  style.fill: "#e8f4f8"

  checkout -> setup-uv -> init-db -> run_pipeline

  init-db: {
    shape: sql_table
    inputs: "env: GITHUB_CONTEXT, GITHUB_TOKEN, CODEX_CONFIG, PR_NUMBER"
    script: "sqlite3 CLI"
    outputs: "config.*, dumps.*"
    state: "DB initialized with all config"
  }
}

Trigger.pull_request -> GHA
Trigger.workflow_dispatch -> GHA

# Secrets
Secrets: {
  GITHUB_TOKEN: "auto-provided by GitHub"

  CODEX_CONFIG: {
    shape: sql_table
    ami_id: "ami-xxx"
    instance_type: "t3.medium"
    key_name: "codex-review"
    security_group_id: "sg-xxx"
    region: "us-east-1"
    ssh_private_key: "-----BEGIN RSA PRIVATE KEY-----..."
    aws_access_key_id: "AKIA..."
    aws_secret_access_key: "secret"
  }
}

Secrets -> GHA.init-db

# Pipeline Scripts with DB I/O
Pipeline: "run_pipeline.py executes in order" {
  style.fill: "#f8f9fa"

  aws_launch_spot: {
    shape: sql_table
    inputs: "ami_id, instance_type, key_name, security_group_id, region, aws_creds"
    script: "aws_launch_spot.py"
    outputs: "instance_id, public_ip"
    state: "EC2 spot instance running"
  }

  ssh_wait: {
    shape: sql_table
    inputs: "public_ip, ssh_private_key"
    script: "ssh_wait.py"
    outputs: "(none)"
    state: "EC2 SSH ready"
  }

  write_agents: {
    shape: sql_table
    inputs: "workdir, pr_number, dumps.github, dumps.github_token"
    script: "write_agents.py"
    outputs: "agents_md, file: AGENTS.md"
    state: "AGENTS.md created locally"
  }

  write_prompt: {
    shape: sql_table
    inputs: "dumps.github"
    script: "write_prompt.py"
    outputs: "prompt"
    state: "prompt ready in DB"
  }

  rsync_to_ec2: {
    shape: sql_table
    inputs: "public_ip, ssh_private_key, workdir"
    script: "rsync_to_ec2.py"
    outputs: "(none)"
    state: "AGENTS.md + prompt.txt synced to EC2 workdir"
  }

  ssh_run_codex: {
    shape: sql_table
    inputs: "public_ip, ssh_private_key, workdir, prompt"
    script: "ssh_run_codex.py"
    outputs: "(none)"
    cmd: "cd {workdir} && codex exec -m gpt-5.2-codex --config model_reasoning_effort=high --dangerously-bypass-approvals-and-sandbox --skip-git-repo-check"
    state: "PR review posted to GitHub"
  }

  ssh_poweroff: {
    shape: sql_table
    inputs: "public_ip, ssh_private_key"
    script: "ssh_poweroff.py"
    outputs: "(none)"
    state: "EC2 instance terminated"
  }

  aws_launch_spot -> ssh_wait -> write_agents -> write_prompt
  write_prompt -> rsync_to_ec2 -> ssh_run_codex -> ssh_poweroff
}

GHA.run_pipeline -> Pipeline

# EC2 Directory Structure
EC2_Dir: "EC2 Directory Structure" {
  shape: sql_table
  pattern: "workdir = /home/ubuntu/{repo_name}/{pr_number}/"
  example: "/home/ubuntu/myapp/42/"
  uploaded: "AGENTS.md, prompt.txt → {workdir}/"
  cloned: "{workdir}/{repo_name}/ (by codex)"
}

# SQLite DB
DB: "SQLite DB (.github/tmp/pipeline.db)" {
  style.fill: "#fff3cd"

  config: {
    shape: sql_table
    workdir: "/home/ubuntu/{reponame}/{pr_number}/"
    pr_number: "string"
    repo: "owner/repo"
    ami_id: "ami-xxx"
    instance_type: "t3.medium"
    key_name: "codex-review"
    security_group_id: "sg-xxx"
    region: "us-east-1"
    ssh_private_key: "RSA key"
    aws_access_key_id: "AKIA..."
    aws_secret_access_key: "secret"
    instance_id: "(by aws_launch_spot)"
    public_ip: "(by aws_launch_spot)"
    prompt: "(by write_prompt)"
    agents_md: "(by write_agents)"
  }

  dumps: {
    shape: sql_table
    github: "json context"
    github_token: "ghp_xxx"
  }
}

GHA.init-db -> DB: "creates"

# AWS EC2
AWS: "AWS EC2 Spot" {
  style.fill: "#ff9900"
  instance: "codex CLI + ~/.codex/auth.json"
}

Pipeline.aws_launch_spot -> AWS.instance: "launches"
Pipeline.rsync_to_ec2 -> EC2_Dir: "uploads to"
Pipeline.ssh_poweroff -> AWS.instance: "poweroff"
EC2_Dir -> AWS.instance: "on"

# Codex Actions
Codex: "Codex on EC2" {
  style.fill: "#d4edda"
  step1: "1. Reads AGENTS.md"
  step2: "2. Clones repo & checkouts PR"
  step3: "3. Reviews diff"
  step4: "4. POSTs review to GitHub API"
  step1 -> step2 -> step3 -> step4
}

AWS.instance -> Codex

# Final Output
PR: "Review on PR" {
  style.fill: "#cce5ff"
}

Codex -> PR

# =============================================================================
# Local Testing (for rapid iteration)
# =============================================================================

LocalTesting: "Local Testing Flow" {
  style.fill: "#f0f0f0"

  trigger: "workflow_dispatch (manual)"

  dump_workflow: "dump-pr-context.yaml" {
    style.fill: "#e8f4f8"
    checkout -> setup-uv -> init-db -> upload-to-ec2
  }

  trigger -> dump_workflow

  dump_details: {
    shape: sql_table
    purpose: "Dumps GHA context to DB and uploads to EC2"
    flow: "init-db → rsync_to_ec2.py"
    ec2_path: "/home/ubuntu/context/{repo_name}/{pr_number}/pipeline.db"
  }

  ec2_upload: {
    shape: sql_table
    script: "rsync_to_ec2.py"
    inputs: "public_ip, ssh_private_key, local pipeline.db"
    outputs: "(none)"
    state: "pipeline.db uploaded to EC2"
  }

  ec2_download: {
    shape: sql_table
    script: "rsync_from_ec2.py"
    inputs: "public_ip, ssh_private_key, ec2_path"
    outputs: ".github/tmp/pipeline.db"
    state: "pipeline.db downloaded locally"
  }

  local_steps: {
    shape: sql_table
    step1: "1. Trigger dump-pr-context workflow (uploads to EC2)"
    step2: "2. Run: uv run rsync_from_ec2.py (downloads pipeline.db)"
    step3: "3. Run scripts locally: uv run .github/codex/{script}.py"
    step4: "4. Or run codex locally to test prompts"
  }

  benefits: {
    shape: sql_table
    fast: "Reuse existing EC2 instance"
    iterate: "Rapid prompt/template iteration"
    debug: "Test scripts individually"
    persistent: "Context stored on EC2, download anytime"
  }

  dump_workflow -> ec2_upload: "runs"
  ec2_download -> local_steps: "then"
}
